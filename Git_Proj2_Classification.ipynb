{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "def Xmodified(X):\n",
    "    #input1  : designmatrix X\n",
    "    #output1 : modified designmatrix --> column containing ones is added\n",
    "    #function modifies the input designmatrix\n",
    "    # column with ones is added \n",
    "    rows = X.shape[0]\n",
    "    onearray = np.ones([1,rows])\n",
    "    Xt = np.transpose(X)\n",
    "    Y=np.concatenate((onearray,Xt))\n",
    "    Z = np.transpose(Y) \n",
    "    return Z\n",
    "\n",
    "def probability(beta,x): \n",
    "    # input1 :  parameter beta\n",
    "    # input2 :  predictors (in our case spin variables)\n",
    "    # output1 : probability for state y = 1 given beta and x \n",
    "    \n",
    "    expo = beta.dot(np.transpose(x))\n",
    "    y = np.exp(expo)/(1+np.exp(expo))\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def matrixW(beta,X):       \n",
    "    #input1 : parameter beta\n",
    "    #input2 : Designmatrix X \n",
    "    #output1: diagonal matrix p(y=1|beta,X[k,:])*p(y=0|beta,X[k,:]) as kth diagonal element\n",
    "    \n",
    "    rows = X.shape[0]\n",
    "    columns = X.shape[1]\n",
    "    \n",
    "    #initializing the diagonal\n",
    "    diagonale = np.zeros([rows])\n",
    "    \n",
    "    #inserting values\n",
    "    for j in range(0,rows):\n",
    "        x = X[j,:]\n",
    "        p = probability(beta,x)    \n",
    "        diagonale[j]= p*(1-p)\n",
    "        \n",
    "    #creating diagonal matrix based on diagonal values\n",
    "    W=np.diag(diagonale)\n",
    "    return W\n",
    "\n",
    "def secondDerivative(beta,X):\n",
    "    #input1 : parameter beta\n",
    "    #input2 : designmatrix X\n",
    "    #output1: second derivative of the cost function (Hessian) \n",
    "    \n",
    "    W = matrixW(beta,X)\n",
    "    Xtranspose = np.transpose(X)\n",
    "    D2 = (Xtranspose.dot(W)).dot(X)\n",
    "    return D2\n",
    "\n",
    "def firstDerivative(beta,X,y):\n",
    "    #input1  : parameter beta\n",
    "    #input2  : designmatrix X\n",
    "    #input3  : target values y\n",
    "    #output1 : first derivative of the cost function\n",
    "    \n",
    "    #generating factor (y-p) as described in lecture slides \n",
    "    length = y.size\n",
    "    factor = np.ones([length])\n",
    "    for k in range(0,length):\n",
    "        x = X[k,:]\n",
    "        prob1 = probability(beta,x)\n",
    "        prob0 = 1 - prob1\n",
    "        factor[k] = y[k] - (y[k]*prob1 + (1-y[k])*prob0)\n",
    "        \n",
    "    D = -np.transpose(X).dot(factor)\n",
    "    return D\n",
    "\n",
    "def NewtonRaphson(X,y,beta,condition,iteration): \n",
    "    #input1 = designmatrix X\n",
    "    #input2 = targetvalues\n",
    "    #input3 = initial parameter beta\n",
    "    #input4 = condition for difference between consecutive betas\n",
    "    #input5 = max iterations\n",
    "    #output1 = optimal parameter beta\n",
    "    \n",
    "    # initialization of betanew and betaold and calculation of needed properties \n",
    "    betanew = beta \n",
    "    betaold = betanew\n",
    "    D2inv   = np.linalg.pinv(secondDerivative(betaold,X))\n",
    "    D1      = firstDerivative(beta,X,y)\n",
    "    subtractor = D2inv.dot(D1)\n",
    "    betanew = betaold - subtractor\n",
    "\n",
    "    # iterate until ||(betaold-betanew)||< condition or we reach max of desired iterations\n",
    "    s = 0\n",
    "    while(s<iteration)and(np.linalg.norm(np.array(betaold)-np.array(betanew))>condition):\n",
    "        betaold=betanew\n",
    "        D2inv = np.linalg.pinv(secondDerivative(betaold,X))\n",
    "        D1 = firstDerivative(beta,X,y)\n",
    "        subtractor = D2inv.dot(D1)\n",
    "        betanew = betaold - subtractor\n",
    "        s = s+1\n",
    "    return betanew\n",
    "\n",
    "def Classifier(beta,x):\n",
    "    #input1 : parameter beta\n",
    "    #input2 : x values (for example from training set)\n",
    "    #output : predicted class (based on soft classifier)\n",
    "    \n",
    "    p = beta.dot(np.transpose(x))\n",
    "    \n",
    "    #because exp(z)/1+exp(z) is monotonously increasing we get\n",
    "    # z>0 => exp(z)/1+exp(z) > 0.5 \n",
    "    if (p>0):\n",
    "        selected_class = 1\n",
    "        return selected_class\n",
    "    if(p<=0):\n",
    "        selected_class = 0\n",
    "        return selected_class\n",
    "    \n",
    "def Accuracy(y,beta,Xtest):\n",
    "    #input1 : target value\n",
    "    #input2 : parameter beta\n",
    "    #input3 : designmatrix of data\n",
    "    #output1: Accuracy score\n",
    "    \n",
    "    #adding column containing ones to the designmatrix\n",
    "    X = Xmodified(Xtest)\n",
    "    \n",
    "    rows = X.shape[0]\n",
    "    columns= X.shape[1]\n",
    "    length = rows\n",
    "    \n",
    "    #initializing ypredict\n",
    "    ypredict = np.zeros(length)\n",
    "    \n",
    "    #filling ypredict with the predicted classes\n",
    "    for j in range(0,length):\n",
    "        ypredict[j] = Classifier(beta,X[j,:])\n",
    "        \n",
    "    #generating vector correct with : correct[k] = 1 if kth predticion is correct\n",
    "    #                                 correct[k] = 0 if kth prediction is incorrect\n",
    "    correct = 1-(abs(y-ypredict))\n",
    "    \n",
    "    #calculating percentage of correct predictions\n",
    "    percentage = sum(correct)/rows\n",
    "    return percentage    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65000 train samples\n",
      "30000 critical samples\n",
      "65000 test samples\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "#Comment this to turn on warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed() # shuffle random seed generator\n",
    "\n",
    "# Ising model parameters\n",
    "L=40 # linear system size\n",
    "J=-1.0 # Ising interaction\n",
    "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
    "T_c=2.26 # Onsager critical temperature in the TD limit\n",
    "\n",
    "##### prepare training and test data sets\n",
    "\n",
    "import pickle,os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###### define ML parameters\n",
    "num_classes=2\n",
    "train_to_test_ratio=0.5 # training samples\n",
    "\n",
    "path_to_data=os.path.expanduser('~')+'/Desktop/Uni/Oslo/MachineLearning/project2/'\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All.pkl\"\n",
    "data = pickle.load(open(path_to_data+file_name,'rb'))\n",
    "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "data=data.astype('int')\n",
    "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\" # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
    "labels = pickle.load(open(path_to_data+file_name,'rb')) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
    "\n",
    "# divide data into ordered, critical and disordered\n",
    "X_ordered=data[:70000,:]\n",
    "Y_ordered=labels[:70000]\n",
    "\n",
    "X_critical=data[70000:100000,:]\n",
    "Y_critical=labels[70000:100000]\n",
    "\n",
    "X_disordered=data[100000:,:]\n",
    "Y_disordered=labels[100000:]\n",
    "\n",
    "del data,labels\n",
    "\n",
    "# define training and test data sets\n",
    "X=np.concatenate((X_ordered,X_disordered))\n",
    "Y=np.concatenate((Y_ordered,Y_disordered))\n",
    "\n",
    "# pick random data points from ordered and disordered states \n",
    "# to create the training and test sets\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio)\n",
    "\n",
    "# full data set\n",
    "X=np.concatenate((X_critical,X))\n",
    "Y=np.concatenate((Y_critical,Y))\n",
    "\n",
    "#print('X_train shape:', X_train.shape)\n",
    "#print('Y_train shape:', Y_train.shape)\n",
    "#print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_critical.shape[0], 'critical samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictors :8\n",
      "Trainsize            :30000\n",
      "Testsize             :60000\n",
      "Accuracy             :0.5294666666666666\n"
     ]
    }
   ],
   "source": [
    "predictors = 8\n",
    "trainsize  = 30000\n",
    "testsize   = 60000\n",
    "X_try = X_train[0:trainsize,0:predictors]\n",
    "Y_try = Y_train[0:trainsize]\n",
    "X = Xmodified(X_try)\n",
    "beta = np.ones([1,predictors+1])\n",
    "betapredict = NewtonRaphson(X,Y_try,beta,0.00001,1)\n",
    "Y_test2 = Y_test[0:testsize]\n",
    "X_test2 = X_test[0:testsize,0:predictors]\n",
    "acc = Accuracy(Y_test2,betapredict,X_test2)\n",
    "\n",
    "print('Number of predictors :'+str(predictors))\n",
    "print('Trainsize            :'+str(trainsize))\n",
    "print('Testsize             :'+str(testsize))\n",
    "print('Accuracy             :'+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
