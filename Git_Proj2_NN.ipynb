{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (65000, 1600)\n",
      "Y_train shape: (65000,)\n",
      "\n",
      "65000 train samples\n",
      "30000 critical samples\n",
      "65000 test samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "#Comment this to turn on warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed() # shuffle random seed generator\n",
    "\n",
    "# Ising model parameters\n",
    "L=40 # linear system size\n",
    "J=-1.0 # Ising interaction\n",
    "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
    "T_c=2.26 # Onsager critical temperature in the TD limit\n",
    "\n",
    "##### prepare training and test data sets\n",
    "\n",
    "import pickle,os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###### define ML parameters\n",
    "num_classes=2\n",
    "train_to_test_ratio=0.5 # training samples\n",
    "\n",
    "path_to_data=os.path.expanduser('~')+'/Desktop/Uni/Oslo/MachineLearning/project2/'\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All.pkl\"\n",
    "data = pickle.load(open(path_to_data+file_name,'rb'))\n",
    "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "data=data.astype('int')\n",
    "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\" # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
    "labels = pickle.load(open(path_to_data+file_name,'rb')) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
    "\n",
    "# divide data into ordered, critical and disordered\n",
    "X_ordered=data[:70000,:]\n",
    "Y_ordered=labels[:70000]\n",
    "\n",
    "X_critical=data[70000:100000,:]\n",
    "Y_critical=labels[70000:100000]\n",
    "\n",
    "X_disordered=data[100000:,:]\n",
    "Y_disordered=labels[100000:]\n",
    "\n",
    "del data,labels\n",
    "\n",
    "# define training and test data sets\n",
    "X=np.concatenate((X_ordered,X_disordered))\n",
    "Y=np.concatenate((Y_ordered,Y_disordered))\n",
    "\n",
    "# pick random data points from ordered and disordered states \n",
    "# to create the training and test sets\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio)\n",
    "\n",
    "# full data set\n",
    "X=np.concatenate((X_critical,X))\n",
    "Y=np.concatenate((Y_critical,Y))\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_critical.shape[0], 'critical samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # building our neural network\n",
    "\n",
    "n_inputs, n_features = X_train.shape\n",
    "n_hidden_neurons = 50\n",
    "n_categories = 2\n",
    "\n",
    "# we make the weights normally distributed using numpy.random.randn\n",
    "\n",
    "# weights and bias in the hidden layer\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "# weights and bias in the output layer\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities = (n_inputs, n_categories) = (65000, 2)\n",
      "probability that image 0 is in category 0,1= \n",
      "[0.98316189 0.01683811]\n",
      "probabilities sum up to: 0.9999999999999999\n",
      "\n",
      "predictions = (n_inputs) = (65000,)\n",
      "prediction for image 0: 0\n",
      "correct label for image 0: 1\n",
      "Old accuracy on training data: 0.3618769230769231\n"
     ]
    }
   ],
   "source": [
    "# setup the feed-forward pass, subscript h = hidden layer\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def feed_forward(X):\n",
    "    # weighted sum of inputs to the hidden layer\n",
    "    z_h = np.matmul(X, hidden_weights) + hidden_bias\n",
    "    # activation in the hidden layer\n",
    "    a_h = sigmoid(z_h)\n",
    "    \n",
    "    # weighted sum of inputs to the output layer\n",
    "    z_o = np.matmul(a_h, output_weights) + output_bias\n",
    "    # softmax output\n",
    "    # axis 0 holds each input and axis 1 the probabilities of each category\n",
    "    exp_term = np.exp(z_o)\n",
    "    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "probabilities = feed_forward(X_train)\n",
    "print(\"probabilities = (n_inputs, n_categories) = \" + str(probabilities.shape))\n",
    "print(\"probability that image 0 is in category 0,1= \\n\" + str(probabilities[0]))\n",
    "print(\"probabilities sum up to: \" + str(probabilities[0].sum()))\n",
    "print()\n",
    "\n",
    "# we obtain a prediction by taking the class with the highest likelihood\n",
    "def predict(X):\n",
    "    probabilities = feed_forward(X)\n",
    "    return np.argmax(probabilities, axis=1)\n",
    "\n",
    "predictions = predict(X_train)\n",
    "print(\"predictions = (n_inputs) = \" + str(predictions.shape))\n",
    "print(\"prediction for image 0: \" + str(predictions[0]))\n",
    "print(\"correct label for image 0: \" + str(Y_train[0]))\n",
    "\n",
    "# to categorical turns our integer vector into a onehot representation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# one-hot in numpy\n",
    "def to_categorical_numpy(integer_vector):\n",
    "    n_inputs = len(integer_vector)\n",
    "    n_categories = np.max(integer_vector) + 1\n",
    "    onehot_vector = np.zeros((n_inputs, n_categories))\n",
    "    onehot_vector[range(n_inputs), integer_vector] = 1\n",
    "    \n",
    "    return onehot_vector\n",
    "\n",
    "#Y_train_onehot, Y_test_onehot = to_categorical(Y_train), to_categorical(Y_test)\n",
    "Y_train_onehot, Y_test_onehot = to_categorical_numpy(Y_train), to_categorical_numpy(Y_test)\n",
    "\n",
    "def feed_forward_train(X):\n",
    "    # weighted sum of inputs to the hidden layer\n",
    "    z_h = np.matmul(X, hidden_weights) + hidden_bias\n",
    "    # activation in the hidden layer\n",
    "    a_h = sigmoid(z_h)\n",
    "    \n",
    "    # weighted sum of inputs to the output layer\n",
    "    z_o = np.matmul(a_h, output_weights) + output_bias\n",
    "    # softmax output\n",
    "    # axis 0 holds each input and axis 1 the probabilities of each category\n",
    "    exp_term = np.exp(z_o)\n",
    "    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "    \n",
    "    # for backpropagation need activations in hidden and output layers\n",
    "    return a_h, probabilities\n",
    "\n",
    "def backpropagation(X, Y):\n",
    "    a_h, probabilities = feed_forward_train(X)\n",
    "    \n",
    "    # error in the output layer\n",
    "    error_output = probabilities - Y\n",
    "    # error in the hidden layer\n",
    "    error_hidden = np.matmul(error_output, output_weights.T) * a_h * (1 - a_h) #sigma L in lecture\n",
    "    \n",
    "    # gradients for the output layer\n",
    "    output_weights_gradient = np.matmul(a_h.T, error_output)\n",
    "    output_bias_gradient = np.sum(error_output, axis=0)\n",
    "    \n",
    "    # gradient for the hidden layer\n",
    "    hidden_weights_gradient = np.matmul(X.T, error_hidden)\n",
    "    hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient, hidden_bias_gradient,error_output\n",
    "\n",
    "print(\"Old accuracy on training data: \" + str(accuracy_score(predict(X_train), Y_train)))\n",
    "score_old_string = str(accuracy_score(predict(X_train), Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New accuracy on training data: 0.4624923076923077\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1          #initializing learning rate eta\n",
    "lmbd = 0.01        #initializing regulation parameter lamda\n",
    "iterations= 10\n",
    "error_old = 300000 # we initialize with a large value to ensure the absolute error in the output layer\n",
    "                   # is smaller\n",
    "\n",
    "for i in range(iterations):\n",
    "    #calculation of gradients and error in the output layer using backpropagation\n",
    "    dWo, dBo, dWh, dBh,error = backpropagation(X_train, Y_train_onehot)\n",
    "    \n",
    "    # case absolute error increased in last iteration --> overshooting occured  \n",
    "    if(np.sum(abs(error_old)<np.sum(abs(error)))):\n",
    "            \n",
    "            # reset weigths and biases -> reverse operation \n",
    "            output_weights = output_weights_old+(eta*dWo_old)\n",
    "            output_bias = output_bias_old+(eta * dBo_old)\n",
    "            hidden_weights = hidden_weights_old+(eta * dWh_old)\n",
    "            hidden_bias = hidden_bias_old+(eta * dBh_old)\n",
    "            \n",
    "            # halving learning rate\n",
    "            eta = 0.5*eta \n",
    "            \n",
    "            # update weights and biases with respect to the new learning rate\n",
    "            output_weights = output_weights-(eta * dWo_old)\n",
    "            output_bias = output_bias-(eta * dBo_old)\n",
    "            hidden_weights =hidden_weights-(eta * dWh_old)\n",
    "            hidden_bias =hidden_bias-(eta * dBh_old)\n",
    "            \n",
    "            #store weights and biases in case we still overshoot with the halved learning rate\n",
    "            output_weights_old = output_weights \n",
    "            output_bias_old = output_bias \n",
    "            hidden_weights_old = hidden_weights\n",
    "            hidden_bias_old = hidden_bias\n",
    "            \n",
    "    # case absolute error decreased in last iteration\n",
    "    # initialization must ensure that else part of if-statement is exercised in first loop\n",
    "    else:\n",
    "            #raising eta\n",
    "            eta = 1.05*eta\n",
    "            \n",
    "             # regularization term gradients\n",
    "            dWo  = dWo + lmbd * output_weights\n",
    "            dWh  = dWh + lmbd * hidden_weights\n",
    "            \n",
    "            # storing weights and biases and gradients in case we overshoot\n",
    "            output_weights_old = output_weights \n",
    "            output_bias_old = output_bias \n",
    "            hidden_weights_old = hidden_weights\n",
    "            hidden_bias_old = hidden_bias\n",
    "            dWo_old =dWo\n",
    "            dBo_old =dBo\n",
    "            dWh_old =dWh\n",
    "            dBh_old =dBh\n",
    "        \n",
    "            # update weights and biases\n",
    "            output_weights = output_weights-(eta * dWo)\n",
    "            output_bias = output_bias-(eta * dBo)\n",
    "            hidden_weights =hidden_weights-(eta * dWh)\n",
    "            hidden_bias =hidden_bias-(eta * dBh)\n",
    "            \n",
    "            #update error\n",
    "            error_old = error   \n",
    "print(\"New accuracy on training data: \" + str(accuracy_score(predict(X_train), Y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old accuracy on training data: 0.3618769230769231\n",
      "New accuracy on training data: 0.4624923076923077\n"
     ]
    }
   ],
   "source": [
    "print(\"Old accuracy on training data: \" + score_old_string)\n",
    "print(\"New accuracy on training data: \" + str(accuracy_score(predict(X_train), Y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
